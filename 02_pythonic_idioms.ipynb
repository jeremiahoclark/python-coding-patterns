{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeremiahoclark/python-coding-patterns/blob/main/02_pythonic_idioms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHW8NvRCLSgN"
      },
      "source": [
        "# Pythonic Idioms and Language-Specific Patterns\n",
        "\n",
        "This notebook covers Python-specific patterns that leverage the language's unique features for clean, idiomatic code. These patterns are not from the Gang of Four but are equally important for writing Pythonic code.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Python has a rich set of idioms and established \"Pythonic\" ways to solve problems. This section covers:\n",
        "- **EAFP vs LBYL**: Error handling philosophy\n",
        "- **Context Managers**: Resource management with `with` statements\n",
        "- **Comprehensions**: Concise collection creation\n",
        "- **Duck Typing**: Polymorphism by protocol\n",
        "- **Iteration Protocol**: Custom iterables and iterators\n",
        "\n",
        "Each pattern includes practical examples with real-world data scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kesa-9dnLSgO"
      },
      "outputs": [],
      "source": [
        "# Required imports for all patterns\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import contextlib\n",
        "from contextlib import contextmanager\n",
        "from typing import Protocol, Iterator, Iterable, Any, Dict, List, Optional\n",
        "from io import StringIO\n",
        "import tempfile\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import threading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMRxfZAOLSgO"
      },
      "source": [
        "## 2.1 EAFP vs LBYL (Coding Style Idiom)\n",
        "\n",
        "**EAFP**: \"Easier to Ask Forgiveness than Permission\" - Try the operation and handle exceptions if it fails.\n",
        "\n",
        "**LBYL**: \"Look Before You Leap\" - Check conditions before performing an action.\n",
        "\n",
        "**Python Philosophy**: EAFP is generally preferred because it's often cleaner and avoids race conditions.\n",
        "\n",
        "**Real-world Example**: Processing configuration data where some keys might be missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ8IQjdnLSgO"
      },
      "outputs": [],
      "source": [
        "# Sample configuration data with missing keys\n",
        "user_configs = [\n",
        "    {\"user_id\": 1, \"name\": \"Alice\", \"email\": \"alice@example.com\", \"age\": 30, \"premium\": True},\n",
        "    {\"user_id\": 2, \"name\": \"Bob\", \"email\": \"bob@example.com\", \"premium\": False},  # Missing age\n",
        "    {\"user_id\": 3, \"name\": \"Charlie\", \"age\": 25},  # Missing email and premium\n",
        "    {\"user_id\": 4, \"name\": \"Diana\", \"email\": \"diana@example.com\", \"age\": 28, \"premium\": True}\n",
        "]\n",
        "\n",
        "def process_user_config_lbyl(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"LBYL approach: Check before accessing.\"\"\"\n",
        "    result = {\"user_id\": config[\"user_id\"], \"name\": config[\"name\"]}\n",
        "\n",
        "    # Check each field before accessing\n",
        "    if \"email\" in config:\n",
        "        result[\"email\"] = config[\"email\"]\n",
        "        result[\"has_email\"] = True\n",
        "    else:\n",
        "        result[\"email\"] = \"not_provided@unknown.com\"\n",
        "        result[\"has_email\"] = False\n",
        "\n",
        "    if \"age\" in config:\n",
        "        result[\"age\"] = config[\"age\"]\n",
        "        result[\"age_group\"] = \"adult\" if config[\"age\"] >= 18 else \"minor\"\n",
        "    else:\n",
        "        result[\"age\"] = 0\n",
        "        result[\"age_group\"] = \"unknown\"\n",
        "\n",
        "    if \"premium\" in config:\n",
        "        result[\"premium\"] = config[\"premium\"]\n",
        "        result[\"account_type\"] = \"premium\" if config[\"premium\"] else \"basic\"\n",
        "    else:\n",
        "        result[\"premium\"] = False\n",
        "        result[\"account_type\"] = \"basic\"\n",
        "\n",
        "    return result\n",
        "\n",
        "def process_user_config_eafp(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"EAFP approach: Try and handle exceptions.\"\"\"\n",
        "    result = {\"user_id\": config[\"user_id\"], \"name\": config[\"name\"]}\n",
        "\n",
        "    # Try to access each field directly\n",
        "    try:\n",
        "        result[\"email\"] = config[\"email\"]\n",
        "        result[\"has_email\"] = True\n",
        "    except KeyError:\n",
        "        result[\"email\"] = \"not_provided@unknown.com\"\n",
        "        result[\"has_email\"] = False\n",
        "\n",
        "    try:\n",
        "        age = config[\"age\"]\n",
        "        result[\"age\"] = age\n",
        "        result[\"age_group\"] = \"adult\" if age >= 18 else \"minor\"\n",
        "    except KeyError:\n",
        "        result[\"age\"] = 0\n",
        "        result[\"age_group\"] = \"unknown\"\n",
        "\n",
        "    try:\n",
        "        premium = config[\"premium\"]\n",
        "        result[\"premium\"] = premium\n",
        "        result[\"account_type\"] = \"premium\" if premium else \"basic\"\n",
        "    except KeyError:\n",
        "        result[\"premium\"] = False\n",
        "        result[\"account_type\"] = \"basic\"\n",
        "\n",
        "    return result\n",
        "\n",
        "def process_user_config_pythonic(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Pythonic approach using dict.get() method.\"\"\"\n",
        "    result = {\"user_id\": config[\"user_id\"], \"name\": config[\"name\"]}\n",
        "\n",
        "    # Use dict.get() with defaults - neither pure LBYL nor EAFP\n",
        "    email = config.get(\"email\", \"not_provided@unknown.com\")\n",
        "    age = config.get(\"age\", 0)\n",
        "    premium = config.get(\"premium\", False)\n",
        "\n",
        "    result.update({\n",
        "        \"email\": email,\n",
        "        \"has_email\": \"@\" in email and email != \"not_provided@unknown.com\",\n",
        "        \"age\": age,\n",
        "        \"age_group\": \"adult\" if age >= 18 else \"minor\" if age > 0 else \"unknown\",\n",
        "        \"premium\": premium,\n",
        "        \"account_type\": \"premium\" if premium else \"basic\"\n",
        "    })\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ntr1Y7dLSgP"
      },
      "outputs": [],
      "source": [
        "# Demo: EAFP vs LBYL\n",
        "print(\"=== EAFP vs LBYL Demo ===\")\n",
        "print(\"Processing user configurations...\\n\")\n",
        "\n",
        "for i, config in enumerate(user_configs, 1):\n",
        "    print(f\"User {i}: {config}\")\n",
        "\n",
        "    # Process with all three approaches\n",
        "    lbyl_result = process_user_config_lbyl(config)\n",
        "    eafp_result = process_user_config_eafp(config)\n",
        "    pythonic_result = process_user_config_pythonic(config)\n",
        "\n",
        "    # Verify all approaches produce the same result\n",
        "    assert lbyl_result == eafp_result == pythonic_result, \"Results should be identical\"\n",
        "\n",
        "    print(f\"  Processed: {pythonic_result}\")\n",
        "    print()\n",
        "\n",
        "# Performance comparison\n",
        "print(\"=== Performance Comparison ===\")\n",
        "\n",
        "# Create test data with varying missing fields\n",
        "test_configs = []\n",
        "for i in range(1000):\n",
        "    config = {\"user_id\": i, \"name\": f\"User{i}\"}\n",
        "    if i % 2 == 0:  # 50% have email\n",
        "        config[\"email\"] = f\"user{i}@example.com\"\n",
        "    if i % 3 == 0:  # 33% have age\n",
        "        config[\"age\"] = random.randint(18, 65)\n",
        "    if i % 4 == 0:  # 25% have premium\n",
        "        config[\"premium\"] = random.choice([True, False])\n",
        "    test_configs.append(config)\n",
        "\n",
        "# Time each approach\n",
        "approaches = [\n",
        "    (\"LBYL\", process_user_config_lbyl),\n",
        "    (\"EAFP\", process_user_config_eafp),\n",
        "    (\"Pythonic\", process_user_config_pythonic)\n",
        "]\n",
        "\n",
        "for name, func in approaches:\n",
        "    start_time = time.time()\n",
        "    results = [func(config) for config in test_configs]\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"{name} approach: {end_time - start_time:.4f} seconds for {len(test_configs)} configs\")\n",
        "\n",
        "print(\"\\nNote: Pythonic approach using dict.get() is often fastest and most readable!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZydnp43LSgP"
      },
      "source": [
        "## 2.2 Context Manager Pattern (`with` statement)\n",
        "\n",
        "**Problem**: Ensure resources are properly cleaned up (files closed, locks released, connections closed) even if exceptions occur.\n",
        "\n",
        "**Solution**: Use context managers with the `with` statement to guarantee cleanup.\n",
        "\n",
        "**Real-world Example**: Managing database connections, temporary files, and performance timing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OacuH5GBLSgP"
      },
      "outputs": [],
      "source": [
        "# Custom context manager class for database simulation\n",
        "class DatabaseConnection:\n",
        "    \"\"\"Simulates a database connection with proper resource management.\"\"\"\n",
        "\n",
        "    def __init__(self, db_name: str, host: str = \"localhost\"):\n",
        "        self.db_name = db_name\n",
        "        self.host = host\n",
        "        self.connection = None\n",
        "        self.transaction_active = False\n",
        "        self.queries_executed = 0\n",
        "\n",
        "    def __enter__(self):\n",
        "        print(f\"🔌 Connecting to database '{self.db_name}' at {self.host}\")\n",
        "        # Simulate connection setup\n",
        "        time.sleep(0.1)  # Simulate connection time\n",
        "        self.connection = f\"conn_to_{self.db_name}\"\n",
        "        print(f\"✅ Connected successfully\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
        "        if exc_type is not None:\n",
        "            print(f\"❌ Exception occurred: {exc_type.__name__}: {exc_value}\")\n",
        "            if self.transaction_active:\n",
        "                print(f\"🔄 Rolling back transaction\")\n",
        "                self.transaction_active = False\n",
        "        else:\n",
        "            if self.transaction_active:\n",
        "                print(f\"✅ Committing transaction\")\n",
        "                self.transaction_active = False\n",
        "\n",
        "        print(f\"🔌 Closing database connection (executed {self.queries_executed} queries)\")\n",
        "        self.connection = None\n",
        "        return False  # Don't suppress exceptions\n",
        "\n",
        "    def begin_transaction(self):\n",
        "        print(f\"🚀 Beginning transaction\")\n",
        "        self.transaction_active = True\n",
        "\n",
        "    def execute_query(self, query: str) -> List[Dict[str, Any]]:\n",
        "        if not self.connection:\n",
        "            raise RuntimeError(\"Not connected to database\")\n",
        "\n",
        "        self.queries_executed += 1\n",
        "        print(f\"📊 Executing query: {query}\")\n",
        "\n",
        "        # Simulate query results\n",
        "        if \"users\" in query.lower():\n",
        "            return [\n",
        "                {\"id\": 1, \"name\": \"Alice\", \"role\": \"admin\"},\n",
        "                {\"id\": 2, \"name\": \"Bob\", \"role\": \"user\"}\n",
        "            ]\n",
        "        elif \"products\" in query.lower():\n",
        "            return [\n",
        "                {\"id\": 1, \"name\": \"Laptop\", \"price\": 999.99},\n",
        "                {\"id\": 2, \"name\": \"Mouse\", \"price\": 29.99}\n",
        "            ]\n",
        "        else:\n",
        "            return [{\"result\": \"success\", \"rows_affected\": 1}]\n",
        "\n",
        "# Function-based context manager using @contextmanager decorator\n",
        "@contextmanager\n",
        "def timing_context(operation_name: str):\n",
        "    \"\"\"Context manager for timing operations.\"\"\"\n",
        "    print(f\"⏱️  Starting {operation_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        yield start_time\n",
        "    finally:\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        print(f\"⏱️  {operation_name} completed in {duration:.4f} seconds\")\n",
        "\n",
        "@contextmanager\n",
        "def temporary_config_file(config_data: Dict[str, Any]):\n",
        "    \"\"\"Context manager for temporary configuration files.\"\"\"\n",
        "    # Create temporary file\n",
        "    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)\n",
        "    temp_path = temp_file.name\n",
        "\n",
        "    try:\n",
        "        # Write config data\n",
        "        json.dump(config_data, temp_file, indent=2)\n",
        "        temp_file.flush()\n",
        "        temp_file.close()\n",
        "\n",
        "        print(f\"📁 Created temporary config file: {temp_path}\")\n",
        "        yield temp_path\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        if os.path.exists(temp_path):\n",
        "            os.unlink(temp_path)\n",
        "            print(f\"🗑️  Removed temporary config file: {temp_path}\")\n",
        "\n",
        "@contextmanager\n",
        "def thread_safe_counter():\n",
        "    \"\"\"Context manager for thread-safe operations.\"\"\"\n",
        "    lock = threading.Lock()\n",
        "    counter = {\"value\": 0}\n",
        "\n",
        "    def increment():\n",
        "        with lock:\n",
        "            counter[\"value\"] += 1\n",
        "            return counter[\"value\"]\n",
        "\n",
        "    def get_value():\n",
        "        with lock:\n",
        "            return counter[\"value\"]\n",
        "\n",
        "    print(\"🔒 Thread-safe counter initialized\")\n",
        "    try:\n",
        "        yield {\"increment\": increment, \"get_value\": get_value}\n",
        "    finally:\n",
        "        print(f\"🔒 Thread-safe counter final value: {counter['value']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KIcaI4lLSgQ"
      },
      "outputs": [],
      "source": [
        "# Demo: Context Manager Pattern\n",
        "print(\"=== Context Manager Pattern Demo ===\")\n",
        "\n",
        "# 1. Database connection with transaction\n",
        "print(\"\\n--- Database Connection Context Manager ---\")\n",
        "try:\n",
        "    with DatabaseConnection(\"sales_db\") as db:\n",
        "        db.begin_transaction()\n",
        "        users = db.execute_query(\"SELECT * FROM users\")\n",
        "        products = db.execute_query(\"SELECT * FROM products\")\n",
        "\n",
        "        print(f\"Found {len(users)} users and {len(products)} products\")\n",
        "\n",
        "        # Simulate some business logic\n",
        "        for user in users:\n",
        "            db.execute_query(f\"UPDATE user_stats SET last_login = NOW() WHERE id = {user['id']}\")\n",
        "\n",
        "        # Transaction will be committed automatically\n",
        "except Exception as e:\n",
        "    print(f\"Database operation failed: {e}\")\n",
        "\n",
        "# 2. Error handling with rollback\n",
        "print(\"\\n--- Database Error Handling ---\")\n",
        "try:\n",
        "    with DatabaseConnection(\"inventory_db\") as db:\n",
        "        db.begin_transaction()\n",
        "        db.execute_query(\"UPDATE inventory SET quantity = quantity - 1 WHERE product_id = 123\")\n",
        "\n",
        "        # Simulate an error\n",
        "        raise ValueError(\"Insufficient inventory!\")\n",
        "\n",
        "        db.execute_query(\"INSERT INTO orders (product_id, quantity) VALUES (123, 1)\")\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Business logic error handled: {e}\")\n",
        "\n",
        "# 3. Timing context manager\n",
        "print(\"\\n--- Timing Context Manager ---\")\n",
        "with timing_context(\"Data Processing\"):\n",
        "    # Simulate some work\n",
        "    data = [i**2 for i in range(10000)]\n",
        "    result = sum(data)\n",
        "    print(f\"Processed {len(data)} items, sum = {result}\")\n",
        "\n",
        "# 4. Temporary file context manager\n",
        "print(\"\\n--- Temporary File Context Manager ---\")\n",
        "config = {\n",
        "    \"database\": {\"host\": \"localhost\", \"port\": 5432, \"name\": \"myapp\"},\n",
        "    \"api\": {\"timeout\": 30, \"retries\": 3},\n",
        "    \"features\": {\"cache_enabled\": True, \"debug_mode\": False}\n",
        "}\n",
        "\n",
        "with temporary_config_file(config) as config_path:\n",
        "    # Read and use the temporary config\n",
        "    with open(config_path, 'r') as f:\n",
        "        loaded_config = json.load(f)\n",
        "\n",
        "    print(f\"Loaded config from {config_path}:\")\n",
        "    print(f\"  Database: {loaded_config['database']['name']}\")\n",
        "    print(f\"  API timeout: {loaded_config['api']['timeout']}s\")\n",
        "    print(f\"  Cache enabled: {loaded_config['features']['cache_enabled']}\")\n",
        "\n",
        "# 5. Thread-safe counter\n",
        "print(\"\\n--- Thread-Safe Counter Context Manager ---\")\n",
        "with thread_safe_counter() as counter:\n",
        "    # Simulate multiple operations\n",
        "    for i in range(5):\n",
        "        new_value = counter[\"increment\"]()\n",
        "        print(f\"  Operation {i+1}: Counter = {new_value}\")\n",
        "\n",
        "    final_value = counter[\"get_value\"]()\n",
        "    print(f\"  Final counter value: {final_value}\")\n",
        "\n",
        "# 6. Multiple context managers\n",
        "print(\"\\n--- Multiple Context Managers ---\")\n",
        "sample_data = {\"results\": [random.randint(1, 100) for _ in range(1000)]}\n",
        "\n",
        "with timing_context(\"Multi-context operation\"), \\\n",
        "     temporary_config_file(sample_data) as temp_file, \\\n",
        "     DatabaseConnection(\"analytics_db\") as db:\n",
        "\n",
        "    # Load data from temp file\n",
        "    with open(temp_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Process data\n",
        "    results = data[\"results\"]\n",
        "    stats = {\n",
        "        \"count\": len(results),\n",
        "        \"mean\": sum(results) / len(results),\n",
        "        \"min\": min(results),\n",
        "        \"max\": max(results)\n",
        "    }\n",
        "\n",
        "    # Store in database\n",
        "    db.execute_query(f\"INSERT INTO analytics (count, mean, min, max) VALUES ({stats['count']}, {stats['mean']:.2f}, {stats['min']}, {stats['max']})\")\n",
        "\n",
        "    print(f\"Processed {stats['count']} values:\")\n",
        "    print(f\"  Mean: {stats['mean']:.2f}\")\n",
        "    print(f\"  Range: {stats['min']} - {stats['max']}\")\n",
        "\n",
        "print(\"\\n✅ All context managers handled resources properly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ8F36HaLSgQ"
      },
      "source": [
        "## 2.3 Comprehensions and Generator Expressions\n",
        "\n",
        "**Problem**: Need concise, readable ways to create collections and process data.\n",
        "\n",
        "**Solution**: Use list, dict, set comprehensions and generator expressions instead of verbose loops.\n",
        "\n",
        "**Real-world Example**: Processing log data, transforming datasets, and analyzing user behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYzZtBNFLSgQ"
      },
      "outputs": [],
      "source": [
        "# Sample data: simulated server logs\n",
        "log_entries = [\n",
        "    {\"timestamp\": \"2024-01-15 10:30:15\", \"level\": \"INFO\", \"message\": \"User login successful\", \"user_id\": 123, \"ip\": \"192.168.1.100\", \"response_time\": 45},\n",
        "    {\"timestamp\": \"2024-01-15 10:31:22\", \"level\": \"ERROR\", \"message\": \"Database connection failed\", \"user_id\": None, \"ip\": \"192.168.1.101\", \"response_time\": 0},\n",
        "    {\"timestamp\": \"2024-01-15 10:32:18\", \"level\": \"INFO\", \"message\": \"API request processed\", \"user_id\": 456, \"ip\": \"192.168.1.102\", \"response_time\": 120},\n",
        "    {\"timestamp\": \"2024-01-15 10:33:05\", \"level\": \"WARN\", \"message\": \"Slow query detected\", \"user_id\": 789, \"ip\": \"192.168.1.103\", \"response_time\": 2500},\n",
        "    {\"timestamp\": \"2024-01-15 10:34:12\", \"level\": \"INFO\", \"message\": \"User logout\", \"user_id\": 123, \"ip\": \"192.168.1.100\", \"response_time\": 25},\n",
        "    {\"timestamp\": \"2024-01-15 10:35:30\", \"level\": \"ERROR\", \"message\": \"Authentication failed\", \"user_id\": None, \"ip\": \"192.168.1.104\", \"response_time\": 15},\n",
        "    {\"timestamp\": \"2024-01-15 10:36:45\", \"level\": \"INFO\", \"message\": \"File upload completed\", \"user_id\": 456, \"ip\": \"192.168.1.102\", \"response_time\": 850},\n",
        "    {\"timestamp\": \"2024-01-15 10:37:20\", \"level\": \"DEBUG\", \"message\": \"Cache miss\", \"user_id\": 789, \"ip\": \"192.168.1.103\", \"response_time\": 5}\n",
        "]\n",
        "\n",
        "# Sample sales data\n",
        "sales_data = [\n",
        "    {\"product\": \"Laptop\", \"category\": \"Electronics\", \"price\": 999.99, \"quantity\": 2, \"discount\": 0.1},\n",
        "    {\"product\": \"Mouse\", \"category\": \"Electronics\", \"price\": 29.99, \"quantity\": 5, \"discount\": 0.0},\n",
        "    {\"product\": \"Keyboard\", \"category\": \"Electronics\", \"price\": 79.99, \"quantity\": 3, \"discount\": 0.05},\n",
        "    {\"product\": \"Book\", \"category\": \"Education\", \"price\": 19.99, \"quantity\": 10, \"discount\": 0.15},\n",
        "    {\"product\": \"Notebook\", \"category\": \"Office\", \"price\": 4.99, \"quantity\": 20, \"discount\": 0.0},\n",
        "    {\"product\": \"Monitor\", \"category\": \"Electronics\", \"price\": 299.99, \"quantity\": 1, \"discount\": 0.2}\n",
        "]\n",
        "\n",
        "def demonstrate_comprehensions():\n",
        "    \"\"\"Demonstrate various comprehension patterns.\"\"\"\n",
        "    print(\"=== Comprehensions Demo ===\")\n",
        "\n",
        "    # 1. List Comprehension: Extract error messages\n",
        "    print(\"\\n--- List Comprehensions ---\")\n",
        "\n",
        "    # Simple filtering and transformation\n",
        "    error_messages = [entry[\"message\"] for entry in log_entries if entry[\"level\"] == \"ERROR\"]\n",
        "    print(f\"Error messages: {error_messages}\")\n",
        "\n",
        "    # Complex transformation with multiple conditions\n",
        "    slow_requests = [\n",
        "        f\"User {entry['user_id']} from {entry['ip']}: {entry['response_time']}ms\"\n",
        "        for entry in log_entries\n",
        "        if entry[\"response_time\"] > 100 and entry[\"user_id\"] is not None\n",
        "    ]\n",
        "    print(f\"Slow requests: {slow_requests}\")\n",
        "\n",
        "    # Nested comprehension: Extract unique IP addresses per log level\n",
        "    ip_by_level = [\n",
        "        (level, [entry[\"ip\"] for entry in log_entries if entry[\"level\"] == level])\n",
        "        for level in set(entry[\"level\"] for entry in log_entries)\n",
        "    ]\n",
        "    print(f\"IPs by level: {dict(ip_by_level)}\")\n",
        "\n",
        "    # 2. Dictionary Comprehensions\n",
        "    print(\"\\n--- Dictionary Comprehensions ---\")\n",
        "\n",
        "    # Create user response time mapping\n",
        "    user_response_times = {\n",
        "        entry[\"user_id\"]: entry[\"response_time\"]\n",
        "        for entry in log_entries\n",
        "        if entry[\"user_id\"] is not None\n",
        "    }\n",
        "    print(f\"User response times: {user_response_times}\")\n",
        "\n",
        "    # Product revenue calculations\n",
        "    product_revenues = {\n",
        "        item[\"product\"]: item[\"price\"] * item[\"quantity\"] * (1 - item[\"discount\"])\n",
        "        for item in sales_data\n",
        "    }\n",
        "    print(f\"Product revenues: {product_revenues}\")\n",
        "\n",
        "    # Category summaries\n",
        "    category_stats = {\n",
        "        category: {\n",
        "            \"items\": len([item for item in sales_data if item[\"category\"] == category]),\n",
        "            \"total_revenue\": sum(\n",
        "                item[\"price\"] * item[\"quantity\"] * (1 - item[\"discount\"])\n",
        "                for item in sales_data if item[\"category\"] == category\n",
        "            )\n",
        "        }\n",
        "        for category in set(item[\"category\"] for item in sales_data)\n",
        "    }\n",
        "    print(f\"Category stats: {category_stats}\")\n",
        "\n",
        "    # 3. Set Comprehensions\n",
        "    print(\"\\n--- Set Comprehensions ---\")\n",
        "\n",
        "    # Unique IP addresses that had errors\n",
        "    error_ips = {entry[\"ip\"] for entry in log_entries if entry[\"level\"] == \"ERROR\"}\n",
        "    print(f\"IPs with errors: {error_ips}\")\n",
        "\n",
        "    # Price ranges (rounded to nearest 10)\n",
        "    price_ranges = {int(item[\"price\"] // 10) * 10 for item in sales_data}\n",
        "    print(f\"Price ranges: {sorted(price_ranges)}\")\n",
        "\n",
        "    # Unique first letters of product names\n",
        "    product_initials = {item[\"product\"][0].upper() for item in sales_data}\n",
        "    print(f\"Product initials: {sorted(product_initials)}\")\n",
        "\n",
        "def demonstrate_generator_expressions():\n",
        "    \"\"\"Demonstrate generator expressions for memory efficiency.\"\"\"\n",
        "    print(\"\\n=== Generator Expressions Demo ===\")\n",
        "\n",
        "    # Memory-efficient processing of large datasets\n",
        "    print(\"\\n--- Memory-Efficient Processing ---\")\n",
        "\n",
        "    # Generator for processing response times\n",
        "    response_time_gen = (\n",
        "        entry[\"response_time\"]\n",
        "        for entry in log_entries\n",
        "        if entry[\"response_time\"] > 0\n",
        "    )\n",
        "\n",
        "    # Calculate statistics without storing all values\n",
        "    valid_times = list(response_time_gen)  # Convert to list for multiple uses\n",
        "    avg_response_time = sum(valid_times) / len(valid_times)\n",
        "    max_response_time = max(valid_times)\n",
        "    min_response_time = min(valid_times)\n",
        "\n",
        "    print(f\"Response time stats:\")\n",
        "    print(f\"  Average: {avg_response_time:.2f}ms\")\n",
        "    print(f\"  Range: {min_response_time}ms - {max_response_time}ms\")\n",
        "\n",
        "    # Generator for revenue calculations\n",
        "    revenue_gen = (\n",
        "        item[\"price\"] * item[\"quantity\"] * (1 - item[\"discount\"])\n",
        "        for item in sales_data\n",
        "    )\n",
        "\n",
        "    total_revenue = sum(revenue_gen)\n",
        "    print(f\"\\nTotal revenue: ${total_revenue:.2f}\")\n",
        "\n",
        "    # Large dataset simulation\n",
        "    print(\"\\n--- Large Dataset Simulation ---\")\n",
        "\n",
        "    # Generate large amount of data lazily\n",
        "    large_dataset_gen = (\n",
        "        {\"id\": i, \"value\": i**2, \"category\": \"A\" if i % 2 == 0 else \"B\"}\n",
        "        for i in range(100000)\n",
        "    )\n",
        "\n",
        "    # Process only what we need\n",
        "    category_a_sum = sum(\n",
        "        item[\"value\"]\n",
        "        for item in large_dataset_gen\n",
        "        if item[\"category\"] == \"A\" and item[\"id\"] < 1000\n",
        "    )\n",
        "\n",
        "    print(f\"Sum of category A values (first 1000): {category_a_sum}\")\n",
        "\n",
        "    # Demonstrate lazy evaluation\n",
        "    print(\"\\n--- Lazy Evaluation Demo ---\")\n",
        "\n",
        "    def expensive_operation(x):\n",
        "        \"\"\"Simulate an expensive operation.\"\"\"\n",
        "        time.sleep(0.001)  # Simulate work\n",
        "        return x * x\n",
        "\n",
        "    # Create generator (no work done yet)\n",
        "    start_time = time.time()\n",
        "    expensive_gen = (expensive_operation(i) for i in range(1000))\n",
        "    creation_time = time.time() - start_time\n",
        "    print(f\"Generator creation time: {creation_time:.4f}s\")\n",
        "\n",
        "    # Use only first 10 values (only 10 operations performed)\n",
        "    start_time = time.time()\n",
        "    first_ten = [next(expensive_gen) for _ in range(10)]\n",
        "    consumption_time = time.time() - start_time\n",
        "    print(f\"Processing first 10 values: {consumption_time:.4f}s\")\n",
        "    print(f\"First 10 squares: {first_ten}\")\n",
        "\n",
        "def demonstrate_advanced_patterns():\n",
        "    \"\"\"Demonstrate advanced comprehension patterns.\"\"\"\n",
        "    print(\"\\n=== Advanced Comprehension Patterns ===\")\n",
        "\n",
        "    # 1. Conditional expressions in comprehensions\n",
        "    print(\"\\n--- Conditional Expressions ---\")\n",
        "\n",
        "    log_summaries = [\n",
        "        f\"{entry['level']}: {entry['message'][:30]}{'...' if len(entry['message']) > 30 else ''}\"\n",
        "        for entry in log_entries\n",
        "    ]\n",
        "    print(\"Log summaries:\")\n",
        "    for summary in log_summaries:\n",
        "        print(f\"  {summary}\")\n",
        "\n",
        "    # 2. Multiple conditions and transformations\n",
        "    print(\"\\n--- Complex Transformations ---\")\n",
        "\n",
        "    enhanced_sales = [\n",
        "        {\n",
        "            **item,\n",
        "            \"final_price\": item[\"price\"] * (1 - item[\"discount\"]),\n",
        "            \"total_value\": item[\"price\"] * item[\"quantity\"] * (1 - item[\"discount\"]),\n",
        "            \"price_tier\": \"premium\" if item[\"price\"] > 100 else \"standard\" if item[\"price\"] > 50 else \"budget\",\n",
        "            \"high_volume\": item[\"quantity\"] > 5\n",
        "        }\n",
        "        for item in sales_data\n",
        "    ]\n",
        "\n",
        "    print(\"Enhanced sales data:\")\n",
        "    for item in enhanced_sales:\n",
        "        print(f\"  {item['product']}: ${item['final_price']:.2f} ({item['price_tier']}) - Total: ${item['total_value']:.2f}\")\n",
        "\n",
        "    # 3. Flattening nested structures\n",
        "    print(\"\\n--- Flattening Nested Data ---\")\n",
        "\n",
        "    nested_data = [\n",
        "        {\"department\": \"Sales\", \"employees\": [\"Alice\", \"Bob\", \"Charlie\"]},\n",
        "        {\"department\": \"Engineering\", \"employees\": [\"David\", \"Eve\", \"Frank\", \"Grace\"]},\n",
        "        {\"department\": \"Marketing\", \"employees\": [\"Henry\", \"Ivy\"]}\n",
        "    ]\n",
        "\n",
        "    # Flatten to list of (employee, department) tuples\n",
        "    employee_dept_pairs = [\n",
        "        (employee, dept[\"department\"])\n",
        "        for dept in nested_data\n",
        "        for employee in dept[\"employees\"]\n",
        "    ]\n",
        "\n",
        "    print(f\"Employee-Department pairs: {employee_dept_pairs}\")\n",
        "\n",
        "    # Create department size mapping\n",
        "    dept_sizes = {\n",
        "        dept[\"department\"]: len(dept[\"employees\"])\n",
        "        for dept in nested_data\n",
        "    }\n",
        "\n",
        "    print(f\"Department sizes: {dept_sizes}\")\n",
        "\n",
        "    # 4. Using walrus operator (:=) in comprehensions (Python 3.8+)\n",
        "    print(\"\\n--- Walrus Operator in Comprehensions ---\")\n",
        "\n",
        "    # Calculate and filter in one pass\n",
        "    high_value_items = [\n",
        "        f\"{item['product']}: ${total_value:.2f}\"\n",
        "        for item in sales_data\n",
        "        if (total_value := item[\"price\"] * item[\"quantity\"] * (1 - item[\"discount\"])) > 500\n",
        "    ]\n",
        "\n",
        "    print(f\"High-value items (>$500): {high_value_items}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwfNrmzNLSgR"
      },
      "outputs": [],
      "source": [
        "# Demo: Comprehensions and Generator Expressions\n",
        "demonstrate_comprehensions()\n",
        "demonstrate_generator_expressions()\n",
        "demonstrate_advanced_patterns()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22yMGSGLSgR"
      },
      "source": [
        "## 2.4 Duck Typing and Protocols\n",
        "\n",
        "**Philosophy**: \"If it looks like a duck and quacks like a duck, it's a duck.\"\n",
        "\n",
        "**Problem**: Need flexible code that works with different types as long as they support required operations.\n",
        "\n",
        "**Solution**: Write code that depends on behavior (methods/attributes) rather than specific types.\n",
        "\n",
        "**Real-world Example**: File processing system that works with different data sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmr3I4RqLSgR"
      },
      "outputs": [],
      "source": [
        "# Duck typing examples with data sources\n",
        "from typing import Protocol, runtime_checkable\n",
        "\n",
        "# Define protocols for static typing (optional but helpful)\n",
        "@runtime_checkable\n",
        "class Readable(Protocol):\n",
        "    \"\"\"Protocol for objects that can be read.\"\"\"\n",
        "    def read(self) -> str: ...\n",
        "    def close(self) -> None: ...\n",
        "\n",
        "@runtime_checkable\n",
        "class DataSource(Protocol):\n",
        "    \"\"\"Protocol for data sources.\"\"\"\n",
        "    def get_data(self) -> List[Dict[str, Any]]: ...\n",
        "    def get_metadata(self) -> Dict[str, Any]: ...\n",
        "\n",
        "class FileDataSource:\n",
        "    \"\"\"Data source that reads from files.\"\"\"\n",
        "\n",
        "    def __init__(self, filename: str):\n",
        "        self.filename = filename\n",
        "        self._data = None\n",
        "\n",
        "    def get_data(self) -> List[Dict[str, Any]]:\n",
        "        if self._data is None:\n",
        "            # Simulate reading from file\n",
        "            self._data = [\n",
        "                {\"id\": 1, \"name\": \"Alice\", \"score\": 95, \"source\": \"file\"},\n",
        "                {\"id\": 2, \"name\": \"Bob\", \"score\": 87, \"source\": \"file\"},\n",
        "                {\"id\": 3, \"name\": \"Charlie\", \"score\": 92, \"source\": \"file\"}\n",
        "            ]\n",
        "        return self._data\n",
        "\n",
        "    def get_metadata(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"source_type\": \"file\",\n",
        "            \"filename\": self.filename,\n",
        "            \"last_modified\": \"2024-01-15 10:30:00\",\n",
        "            \"size_bytes\": 256\n",
        "        }\n",
        "\n",
        "class DatabaseDataSource:\n",
        "    \"\"\"Data source that reads from database.\"\"\"\n",
        "\n",
        "    def __init__(self, connection_string: str):\n",
        "        self.connection_string = connection_string\n",
        "        self._cached_data = None\n",
        "\n",
        "    def get_data(self) -> List[Dict[str, Any]]:\n",
        "        if self._cached_data is None:\n",
        "            # Simulate database query\n",
        "            self._cached_data = [\n",
        "                {\"id\": 4, \"name\": \"Diana\", \"score\": 98, \"source\": \"database\"},\n",
        "                {\"id\": 5, \"name\": \"Eve\", \"score\": 89, \"source\": \"database\"},\n",
        "                {\"id\": 6, \"name\": \"Frank\", \"score\": 94, \"source\": \"database\"}\n",
        "            ]\n",
        "        return self._cached_data\n",
        "\n",
        "    def get_metadata(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"source_type\": \"database\",\n",
        "            \"connection\": self.connection_string,\n",
        "            \"last_query\": \"2024-01-15 10:35:00\",\n",
        "            \"rows_returned\": len(self.get_data())\n",
        "        }\n",
        "\n",
        "class APIDataSource:\n",
        "    \"\"\"Data source that reads from API.\"\"\"\n",
        "\n",
        "    def __init__(self, api_url: str, api_key: str):\n",
        "        self.api_url = api_url\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def get_data(self) -> List[Dict[str, Any]]:\n",
        "        # Simulate API call\n",
        "        return [\n",
        "            {\"id\": 7, \"name\": \"Grace\", \"score\": 91, \"source\": \"api\"},\n",
        "            {\"id\": 8, \"name\": \"Henry\", \"score\": 86, \"source\": \"api\"}\n",
        "        ]\n",
        "\n",
        "    def get_metadata(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"source_type\": \"api\",\n",
        "            \"endpoint\": self.api_url,\n",
        "            \"api_version\": \"v2.1\",\n",
        "            \"rate_limit\": \"1000/hour\"\n",
        "        }\n",
        "\n",
        "class MockDataSource:\n",
        "    \"\"\"Mock data source for testing.\"\"\"\n",
        "\n",
        "    def __init__(self, mock_data: List[Dict[str, Any]]):\n",
        "        self.mock_data = mock_data\n",
        "\n",
        "    def get_data(self) -> List[Dict[str, Any]]:\n",
        "        return self.mock_data\n",
        "\n",
        "    def get_metadata(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"source_type\": \"mock\",\n",
        "            \"records_count\": len(self.mock_data),\n",
        "            \"created_at\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Duck typing in action - functions that work with any \"data source\"\n",
        "def analyze_data(data_source) -> Dict[str, Any]:\n",
        "    \"\"\"Analyze data from any source that implements the DataSource protocol.\"\"\"\n",
        "    # Duck typing: we don't check the type, just use the methods\n",
        "    try:\n",
        "        data = data_source.get_data()\n",
        "        metadata = data_source.get_metadata()\n",
        "\n",
        "        # Perform analysis\n",
        "        scores = [item.get(\"score\", 0) for item in data if \"score\" in item]\n",
        "\n",
        "        analysis = {\n",
        "            \"source_info\": metadata,\n",
        "            \"record_count\": len(data),\n",
        "            \"has_scores\": len(scores) > 0\n",
        "        }\n",
        "\n",
        "        if scores:\n",
        "            analysis.update({\n",
        "                \"average_score\": sum(scores) / len(scores),\n",
        "                \"max_score\": max(scores),\n",
        "                \"min_score\": min(scores),\n",
        "                \"score_distribution\": {\n",
        "                    \"excellent\": len([s for s in scores if s >= 95]),\n",
        "                    \"good\": len([s for s in scores if 85 <= s < 95]),\n",
        "                    \"fair\": len([s for s in scores if s < 85])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except AttributeError as e:\n",
        "        return {\"error\": f\"Object doesn't implement required methods: {e}\"}\n",
        "\n",
        "def merge_data_sources(*sources) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Merge data from multiple sources.\"\"\"\n",
        "    merged_data = []\n",
        "    source_stats = []\n",
        "\n",
        "    for i, source in enumerate(sources):\n",
        "        try:\n",
        "            data = source.get_data()\n",
        "            metadata = source.get_metadata()\n",
        "\n",
        "            # Add source identifier to each record\n",
        "            for record in data:\n",
        "                record[\"source_index\"] = i\n",
        "                record[\"source_type\"] = metadata.get(\"source_type\", \"unknown\")\n",
        "\n",
        "            merged_data.extend(data)\n",
        "            source_stats.append({\n",
        "                \"index\": i,\n",
        "                \"type\": metadata.get(\"source_type\", \"unknown\"),\n",
        "                \"records\": len(data)\n",
        "            })\n",
        "\n",
        "        except AttributeError as e:\n",
        "            print(f\"Warning: Source {i} doesn't implement required methods: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Merged data from {len(source_stats)} sources: {source_stats}\")\n",
        "    return merged_data\n",
        "\n",
        "# File-like objects with duck typing\n",
        "class StringDataFile:\n",
        "    \"\"\"String-based file-like object.\"\"\"\n",
        "\n",
        "    def __init__(self, content: str):\n",
        "        self.content = content\n",
        "        self.position = 0\n",
        "        self.closed = False\n",
        "\n",
        "    def read(self, size: int = -1) -> str:\n",
        "        if self.closed:\n",
        "            raise ValueError(\"I/O operation on closed file\")\n",
        "\n",
        "        if size == -1:\n",
        "            result = self.content[self.position:]\n",
        "            self.position = len(self.content)\n",
        "        else:\n",
        "            result = self.content[self.position:self.position + size]\n",
        "            self.position += len(result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def readline(self) -> str:\n",
        "        if self.closed:\n",
        "            raise ValueError(\"I/O operation on closed file\")\n",
        "\n",
        "        newline_pos = self.content.find('\\n', self.position)\n",
        "        if newline_pos == -1:\n",
        "            result = self.content[self.position:]\n",
        "            self.position = len(self.content)\n",
        "        else:\n",
        "            result = self.content[self.position:newline_pos + 1]\n",
        "            self.position = newline_pos + 1\n",
        "\n",
        "        return result\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.closed = True\n",
        "\n",
        "class LogFile:\n",
        "    \"\"\"Log file that adds timestamps.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.lines = []\n",
        "        self.position = 0\n",
        "        self.closed = False\n",
        "\n",
        "    def write(self, text: str) -> None:\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.lines.append(f\"[{timestamp}] {text}\")\n",
        "\n",
        "    def read(self, size: int = -1) -> str:\n",
        "        content = \"\\n\".join(self.lines)\n",
        "        if size == -1:\n",
        "            return content\n",
        "        else:\n",
        "            return content[:size]\n",
        "\n",
        "    def readline(self) -> str:\n",
        "        if self.position < len(self.lines):\n",
        "            line = self.lines[self.position] + \"\\n\"\n",
        "            self.position += 1\n",
        "            return line\n",
        "        return \"\"\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.closed = True\n",
        "\n",
        "def process_file_like_object(file_obj) -> Dict[str, Any]:\n",
        "    \"\"\"Process any file-like object using duck typing.\"\"\"\n",
        "    try:\n",
        "        # Try to read the entire content\n",
        "        content = file_obj.read()\n",
        "\n",
        "        # Analyze the content\n",
        "        lines = content.split('\\n')\n",
        "        words = content.split()\n",
        "\n",
        "        analysis = {\n",
        "            \"total_characters\": len(content),\n",
        "            \"total_lines\": len(lines),\n",
        "            \"total_words\": len(words),\n",
        "            \"average_line_length\": sum(len(line) for line in lines) / len(lines) if lines else 0,\n",
        "            \"has_timestamps\": any(\"[\" in line and \"]\" in line for line in lines)\n",
        "        }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except AttributeError as e:\n",
        "        return {\"error\": f\"Object doesn't behave like a file: {e}\"}\n",
        "    finally:\n",
        "        # Try to close if the object supports it\n",
        "        if hasattr(file_obj, 'close'):\n",
        "            file_obj.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_A48FseLSgR"
      },
      "outputs": [],
      "source": [
        "# Demo: Duck Typing and Protocols\n",
        "print(\"=== Duck Typing and Protocols Demo ===\")\n",
        "\n",
        "# 1. Data source duck typing\n",
        "print(\"\\n--- Data Source Duck Typing ---\")\n",
        "\n",
        "# Create different data sources\n",
        "file_source = FileDataSource(\"students.json\")\n",
        "db_source = DatabaseDataSource(\"postgresql://localhost/school\")\n",
        "api_source = APIDataSource(\"https://api.school.com/students\", \"key123\")\n",
        "mock_source = MockDataSource([\n",
        "    {\"id\": 9, \"name\": \"Ivy\", \"score\": 93, \"source\": \"mock\"},\n",
        "    {\"id\": 10, \"name\": \"Jack\", \"score\": 88, \"source\": \"mock\"}\n",
        "])\n",
        "\n",
        "# Analyze each source using duck typing\n",
        "sources = [\n",
        "    (\"File Source\", file_source),\n",
        "    (\"Database Source\", db_source),\n",
        "    (\"API Source\", api_source),\n",
        "    (\"Mock Source\", mock_source)\n",
        "]\n",
        "\n",
        "for name, source in sources:\n",
        "    print(f\"\\n{name}:\")\n",
        "    analysis = analyze_data(source)\n",
        "\n",
        "    if \"error\" in analysis:\n",
        "        print(f\"  Error: {analysis['error']}\")\n",
        "    else:\n",
        "        print(f\"  Type: {analysis['source_info']['source_type']}\")\n",
        "        print(f\"  Records: {analysis['record_count']}\")\n",
        "        if analysis['has_scores']:\n",
        "            print(f\"  Average Score: {analysis['average_score']:.1f}\")\n",
        "            print(f\"  Score Range: {analysis['min_score']} - {analysis['max_score']}\")\n",
        "            dist = analysis['score_distribution']\n",
        "            print(f\"  Distribution: {dist['excellent']} excellent, {dist['good']} good, {dist['fair']} fair\")\n",
        "\n",
        "# 2. Merge multiple data sources\n",
        "print(\"\\n--- Merging Multiple Data Sources ---\")\n",
        "all_data = merge_data_sources(file_source, db_source, api_source, mock_source)\n",
        "\n",
        "print(f\"\\nMerged data ({len(all_data)} total records):\")\n",
        "for record in all_data:\n",
        "    print(f\"  {record['name']} (score: {record['score']}, from: {record['source_type']})\")\n",
        "\n",
        "# 3. Protocol checking (runtime)\n",
        "print(\"\\n--- Protocol Checking ---\")\n",
        "for name, source in sources:\n",
        "    is_data_source = isinstance(source, DataSource)\n",
        "    print(f\"{name} implements DataSource protocol: {is_data_source}\")\n",
        "\n",
        "# 4. File-like object duck typing\n",
        "print(\"\\n--- File-like Object Duck Typing ---\")\n",
        "\n",
        "# Create different file-like objects\n",
        "string_file = StringDataFile(\"Hello World\\nThis is line 2\\nAnd line 3\")\n",
        "log_file = LogFile()\n",
        "log_file.write(\"System started\")\n",
        "log_file.write(\"User logged in\")\n",
        "log_file.write(\"Processing request\")\n",
        "\n",
        "# Use StringIO for comparison\n",
        "string_io = StringIO(\"StringIO content\\nLine 2 from StringIO\\nEnd of StringIO\")\n",
        "\n",
        "file_objects = [\n",
        "    (\"String File\", string_file),\n",
        "    (\"Log File\", log_file),\n",
        "    (\"StringIO\", string_io)\n",
        "]\n",
        "\n",
        "for name, file_obj in file_objects:\n",
        "    print(f\"\\n{name}:\")\n",
        "    analysis = process_file_like_object(file_obj)\n",
        "\n",
        "    if \"error\" in analysis:\n",
        "        print(f\"  Error: {analysis['error']}\")\n",
        "    else:\n",
        "        print(f\"  Characters: {analysis['total_characters']}\")\n",
        "        print(f\"  Lines: {analysis['total_lines']}\")\n",
        "        print(f\"  Words: {analysis['total_words']}\")\n",
        "        print(f\"  Avg line length: {analysis['average_line_length']:.1f}\")\n",
        "        print(f\"  Has timestamps: {analysis['has_timestamps']}\")\n",
        "\n",
        "# 5. Duck typing failure example\n",
        "print(\"\\n--- Duck Typing Failure Example ---\")\n",
        "\n",
        "class NotADataSource:\n",
        "    \"\"\"Class that doesn't implement the DataSource protocol.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.value = 42\n",
        "\n",
        "    def get_value(self):  # Wrong method name\n",
        "        return self.value\n",
        "\n",
        "fake_source = NotADataSource()\n",
        "analysis = analyze_data(fake_source)\n",
        "print(f\"Analysis of non-conforming object: {analysis}\")\n",
        "\n",
        "print(\"\\n✅ Duck typing allows flexible, reusable code that works with any object implementing the expected interface!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XMTguFPLSgR"
      },
      "source": [
        "## 2.5 Iteration Protocol and Iterable Pattern\n",
        "\n",
        "**Problem**: Need to create custom objects that can be used in `for` loops, comprehensions, and other iteration contexts.\n",
        "\n",
        "**Solution**: Implement the iteration protocol by defining `__iter__` and `__next__` methods.\n",
        "\n",
        "**Real-world Example**: Custom data structures, pagination, and lazy data generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfMXrY5ULSgR"
      },
      "outputs": [],
      "source": [
        "# Iteration protocol examples\n",
        "from typing import Iterator, Iterable, Generator\n",
        "\n",
        "class NumberSequence:\n",
        "    \"\"\"Simple iterable that generates a sequence of numbers.\"\"\"\n",
        "\n",
        "    def __init__(self, start: int, end: int, step: int = 1):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.step = step\n",
        "\n",
        "    def __iter__(self) -> Iterator[int]:\n",
        "        \"\"\"Return an iterator (using generator for simplicity).\"\"\"\n",
        "        current = self.start\n",
        "        while current < self.end:\n",
        "            yield current\n",
        "            current += self.step\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Calculate length of the sequence.\"\"\"\n",
        "        return max(0, (self.end - self.start + self.step - 1) // self.step)\n",
        "\n",
        "class DataBatch:\n",
        "    \"\"\"Iterator for processing data in batches.\"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Any], batch_size: int):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.current_index = 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[Any]]:\n",
        "        return self\n",
        "\n",
        "    def __next__(self) -> List[Any]:\n",
        "        if self.current_index >= len(self.data):\n",
        "            raise StopIteration\n",
        "\n",
        "        batch = self.data[self.current_index:self.current_index + self.batch_size]\n",
        "        self.current_index += self.batch_size\n",
        "        return batch\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Reset iterator to beginning.\"\"\"\n",
        "        self.current_index = 0\n",
        "\n",
        "class PaginatedAPI:\n",
        "    \"\"\"Simulates paginated API responses with automatic pagination.\"\"\"\n",
        "\n",
        "    def __init__(self, total_items: int, page_size: int = 10):\n",
        "        self.total_items = total_items\n",
        "        self.page_size = page_size\n",
        "        self.total_pages = (total_items + page_size - 1) // page_size\n",
        "\n",
        "    def __iter__(self) -> Iterator[Dict[str, Any]]:\n",
        "        \"\"\"Iterate through all pages automatically.\"\"\"\n",
        "        for page_num in range(1, self.total_pages + 1):\n",
        "            page_data = self._fetch_page(page_num)\n",
        "            for item in page_data[\"items\"]:\n",
        "                yield item\n",
        "\n",
        "    def _fetch_page(self, page_num: int) -> Dict[str, Any]:\n",
        "        \"\"\"Simulate API call to fetch a page.\"\"\"\n",
        "        start_idx = (page_num - 1) * self.page_size\n",
        "        end_idx = min(start_idx + self.page_size, self.total_items)\n",
        "\n",
        "        items = [\n",
        "            {\n",
        "                \"id\": i,\n",
        "                \"name\": f\"Item {i}\",\n",
        "                \"value\": i * 10,\n",
        "                \"page\": page_num\n",
        "            }\n",
        "            for i in range(start_idx, end_idx)\n",
        "        ]\n",
        "\n",
        "        # Simulate API response structure\n",
        "        return {\n",
        "            \"page\": page_num,\n",
        "            \"page_size\": self.page_size,\n",
        "            \"total_pages\": self.total_pages,\n",
        "            \"total_items\": self.total_items,\n",
        "            \"items\": items\n",
        "        }\n",
        "\n",
        "    def get_pages(self) -> Iterator[Dict[str, Any]]:\n",
        "        \"\"\"Iterator that yields complete pages instead of individual items.\"\"\"\n",
        "        for page_num in range(1, self.total_pages + 1):\n",
        "            yield self._fetch_page(page_num)\n",
        "\n",
        "class FibonacciSequence:\n",
        "    \"\"\"Fibonacci sequence generator with different iteration modes.\"\"\"\n",
        "\n",
        "    def __init__(self, max_count: Optional[int] = None, max_value: Optional[int] = None):\n",
        "        self.max_count = max_count\n",
        "        self.max_value = max_value\n",
        "\n",
        "    def __iter__(self) -> Iterator[int]:\n",
        "        \"\"\"Generate Fibonacci numbers up to specified limit.\"\"\"\n",
        "        a, b = 0, 1\n",
        "        count = 0\n",
        "\n",
        "        while True:\n",
        "            # Check count limit\n",
        "            if self.max_count is not None and count >= self.max_count:\n",
        "                break\n",
        "\n",
        "            # Check value limit\n",
        "            if self.max_value is not None and a > self.max_value:\n",
        "                break\n",
        "\n",
        "            yield a\n",
        "            a, b = b, a + b\n",
        "            count += 1\n",
        "\n",
        "    def pairs(self) -> Iterator[tuple[int, int]]:\n",
        "        \"\"\"Generate consecutive Fibonacci pairs.\"\"\"\n",
        "        fib_iter = iter(self)\n",
        "        try:\n",
        "            prev = next(fib_iter)\n",
        "            for current in fib_iter:\n",
        "                yield (prev, current)\n",
        "                prev = current\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "    def ratios(self) -> Iterator[float]:\n",
        "        \"\"\"Generate ratios between consecutive Fibonacci numbers (approaches golden ratio).\"\"\"\n",
        "        for prev, current in self.pairs():\n",
        "            if prev != 0:\n",
        "                yield current / prev\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Processes data with lazy evaluation and caching.\"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Dict[str, Any]]):\n",
        "        self.data = data\n",
        "        self._cache = {}\n",
        "\n",
        "    def filter_by_category(self, category: str) -> Iterator[Dict[str, Any]]:\n",
        "        \"\"\"Filter items by category with lazy evaluation.\"\"\"\n",
        "        cache_key = f\"category_{category}\"\n",
        "\n",
        "        if cache_key not in self._cache:\n",
        "            print(f\"🔍 Filtering by category: {category}\")\n",
        "            self._cache[cache_key] = [\n",
        "                item for item in self.data\n",
        "                if item.get(\"category\") == category\n",
        "            ]\n",
        "\n",
        "        for item in self._cache[cache_key]:\n",
        "            yield item\n",
        "\n",
        "    def transform_values(self, transform_func) -> Iterator[Dict[str, Any]]:\n",
        "        \"\"\"Apply transformation to each item.\"\"\"\n",
        "        for item in self.data:\n",
        "            yield transform_func(item)\n",
        "\n",
        "    def sliding_window(self, window_size: int) -> Iterator[List[Dict[str, Any]]]:\n",
        "        \"\"\"Generate sliding windows of data.\"\"\"\n",
        "        if window_size <= 0:\n",
        "            return\n",
        "\n",
        "        for i in range(len(self.data) - window_size + 1):\n",
        "            yield self.data[i:i + window_size]\n",
        "\n",
        "# Generator functions for common patterns\n",
        "def infinite_counter(start: int = 0, step: int = 1) -> Generator[int, None, None]:\n",
        "    \"\"\"Infinite counter generator.\"\"\"\n",
        "    current = start\n",
        "    while True:\n",
        "        yield current\n",
        "        current += step\n",
        "\n",
        "def chunked(iterable: Iterable[Any], chunk_size: int) -> Generator[List[Any], None, None]:\n",
        "    \"\"\"Split iterable into chunks of specified size.\"\"\"\n",
        "    iterator = iter(iterable)\n",
        "    while True:\n",
        "        chunk = list(itertools.islice(iterator, chunk_size))\n",
        "        if not chunk:\n",
        "            break\n",
        "        yield chunk\n",
        "\n",
        "def enumerate_with_step(iterable: Iterable[Any], start: int = 0, step: int = 1) -> Generator[tuple[int, Any], None, None]:\n",
        "    \"\"\"Enhanced enumerate with custom step.\"\"\"\n",
        "    index = start\n",
        "    for item in iterable:\n",
        "        yield (index, item)\n",
        "        index += step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2QQML7uLSgS"
      },
      "outputs": [],
      "source": [
        "# Demo: Iteration Protocol and Iterable Pattern\n",
        "print(\"=== Iteration Protocol and Iterable Pattern Demo ===\")\n",
        "\n",
        "# 1. Custom number sequence\n",
        "print(\"\\n--- Custom Number Sequence ---\")\n",
        "seq = NumberSequence(2, 20, 3)\n",
        "print(f\"Sequence from 2 to 20 with step 3: {list(seq)}\")\n",
        "print(f\"Length: {len(seq)}\")\n",
        "\n",
        "# Use in comprehension\n",
        "squares = [x**2 for x in NumberSequence(1, 6)]\n",
        "print(f\"Squares: {squares}\")\n",
        "\n",
        "# 2. Data batching\n",
        "print(\"\\n--- Data Batching ---\")\n",
        "sample_data = list(range(25))  # 0 to 24\n",
        "batch_processor = DataBatch(sample_data, batch_size=7)\n",
        "\n",
        "print(\"Processing data in batches of 7:\")\n",
        "for i, batch in enumerate(batch_processor, 1):\n",
        "    print(f\"  Batch {i}: {batch}\")\n",
        "\n",
        "# Reset and process again\n",
        "print(\"\\nReset and process first 2 batches:\")\n",
        "batch_processor.reset()\n",
        "for i, batch in enumerate(batch_processor, 1):\n",
        "    print(f\"  Batch {i}: {batch}\")\n",
        "    if i >= 2:\n",
        "        break\n",
        "\n",
        "# 3. Paginated API simulation\n",
        "print(\"\\n--- Paginated API ---\")\n",
        "api = PaginatedAPI(total_items=35, page_size=8)\n",
        "\n",
        "# Iterate through all items automatically\n",
        "print(\"First 10 items from paginated API:\")\n",
        "for i, item in enumerate(api, 1):\n",
        "    print(f\"  {item['name']} (ID: {item['id']}, Value: {item['value']}, Page: {item['page']})\")\n",
        "    if i >= 10:\n",
        "        break\n",
        "\n",
        "# Iterate through pages\n",
        "print(\"\\nPage-by-page iteration:\")\n",
        "for page in api.get_pages():\n",
        "    print(f\"  Page {page['page']}: {len(page['items'])} items (IDs: {[item['id'] for item in page['items']]})\")\n",
        "\n",
        "# 4. Fibonacci sequence with different modes\n",
        "print(\"\\n--- Fibonacci Sequence ---\")\n",
        "\n",
        "# First 10 Fibonacci numbers\n",
        "fib_count = FibonacciSequence(max_count=10)\n",
        "print(f\"First 10 Fibonacci numbers: {list(fib_count)}\")\n",
        "\n",
        "# Fibonacci numbers up to 100\n",
        "fib_value = FibonacciSequence(max_value=100)\n",
        "print(f\"Fibonacci numbers ≤ 100: {list(fib_value)}\")\n",
        "\n",
        "# Fibonacci pairs\n",
        "fib_pairs = FibonacciSequence(max_count=8)\n",
        "print(f\"Fibonacci pairs: {list(fib_pairs.pairs())}\")\n",
        "\n",
        "# Fibonacci ratios (approaching golden ratio)\n",
        "fib_ratios = FibonacciSequence(max_count=10)\n",
        "ratios = list(fib_ratios.ratios())\n",
        "print(f\"Fibonacci ratios: {[f'{ratio:.6f}' for ratio in ratios]}\")\n",
        "print(f\"Last ratio (≈ golden ratio): {ratios[-1]:.10f}\")\n",
        "\n",
        "# 5. Data processing with lazy evaluation\n",
        "print(\"\\n--- Data Processing with Lazy Evaluation ---\")\n",
        "\n",
        "# Sample dataset\n",
        "dataset = [\n",
        "    {\"id\": 1, \"name\": \"Product A\", \"category\": \"Electronics\", \"price\": 99.99, \"rating\": 4.5},\n",
        "    {\"id\": 2, \"name\": \"Product B\", \"category\": \"Books\", \"price\": 19.99, \"rating\": 4.2},\n",
        "    {\"id\": 3, \"name\": \"Product C\", \"category\": \"Electronics\", \"price\": 199.99, \"rating\": 4.8},\n",
        "    {\"id\": 4, \"name\": \"Product D\", \"category\": \"Clothing\", \"price\": 49.99, \"rating\": 3.9},\n",
        "    {\"id\": 5, \"name\": \"Product E\", \"category\": \"Electronics\", \"price\": 299.99, \"rating\": 4.7},\n",
        "    {\"id\": 6, \"name\": \"Product F\", \"category\": \"Books\", \"price\": 24.99, \"rating\": 4.1}\n",
        "]\n",
        "\n",
        "processor = DataProcessor(dataset)\n",
        "\n",
        "# Filter by category (cached)\n",
        "print(\"Electronics products:\")\n",
        "electronics = list(processor.filter_by_category(\"Electronics\"))\n",
        "for product in electronics:\n",
        "    print(f\"  {product['name']}: ${product['price']} (Rating: {product['rating']})\")\n",
        "\n",
        "# Second call uses cache\n",
        "print(\"\\nSecond call to Electronics filter (uses cache):\")\n",
        "electronics_cached = list(processor.filter_by_category(\"Electronics\"))\n",
        "print(f\"Found {len(electronics_cached)} electronics products\")\n",
        "\n",
        "# Transform values\n",
        "def add_price_tier(item):\n",
        "    item = item.copy()\n",
        "    price = item['price']\n",
        "    if price < 50:\n",
        "        item['price_tier'] = 'Budget'\n",
        "    elif price < 150:\n",
        "        item['price_tier'] = 'Mid-range'\n",
        "    else:\n",
        "        item['price_tier'] = 'Premium'\n",
        "    return item\n",
        "\n",
        "print(\"\\nProducts with price tiers:\")\n",
        "enhanced_products = list(processor.transform_values(add_price_tier))\n",
        "for product in enhanced_products:\n",
        "    print(f\"  {product['name']}: {product['price_tier']} (${product['price']})\")\n",
        "\n",
        "# Sliding window\n",
        "print(\"\\nSliding window of size 3:\")\n",
        "for i, window in enumerate(processor.sliding_window(3), 1):\n",
        "    window_names = [item['name'] for item in window]\n",
        "    print(f\"  Window {i}: {window_names}\")\n",
        "\n",
        "# 6. Generator functions\n",
        "print(\"\\n--- Generator Functions ---\")\n",
        "\n",
        "# Infinite counter (take first 5)\n",
        "counter = infinite_counter(10, 3)\n",
        "first_five = [next(counter) for _ in range(5)]\n",
        "print(f\"First 5 from infinite counter (start=10, step=3): {first_five}\")\n",
        "\n",
        "# Enhanced enumerate\n",
        "items = ['apple', 'banana', 'cherry', 'date']\n",
        "enumerated = list(enumerate_with_step(items, start=100, step=5))\n",
        "print(f\"Enhanced enumerate: {enumerated}\")\n",
        "\n",
        "print(\"\\n✅ Custom iterables integrate seamlessly with Python's iteration ecosystem!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUp5m3d6LSgS"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated 5 essential Pythonic idioms and language-specific patterns:\n",
        "\n",
        "### 1. **EAFP vs LBYL**\n",
        "- **Use Case**: Error handling and conditional logic\n",
        "- **Key Learning**: Python favors \"try and handle exceptions\" over \"check first\"\n",
        "- **Demo**: Configuration processing with missing fields\n",
        "\n",
        "### 2. **Context Managers**\n",
        "- **Use Case**: Resource management and guaranteed cleanup\n",
        "- **Key Learning**: `with` statements ensure proper resource handling\n",
        "- **Demo**: Database connections, temporary files, and timing operations\n",
        "\n",
        "### 3. **Comprehensions and Generator Expressions**\n",
        "- **Use Case**: Concise collection creation and data transformation\n",
        "- **Key Learning**: More readable and often faster than explicit loops\n",
        "- **Demo**: Log analysis, sales data processing, and memory-efficient computations\n",
        "\n",
        "### 4. **Duck Typing and Protocols**\n",
        "- **Use Case**: Flexible, polymorphic code based on behavior\n",
        "- **Key Learning**: \"If it quacks like a duck, it's a duck\"\n",
        "- **Demo**: Data sources, file-like objects, and protocol-based design\n",
        "\n",
        "### 5. **Iteration Protocol**\n",
        "- **Use Case**: Custom iterables and lazy data generation\n",
        "- **Key Learning**: Implement `__iter__` and `__next__` for seamless integration\n",
        "- **Demo**: Number sequences, pagination, Fibonacci generation, and data processing\n",
        "\n",
        "### Key Takeaways\n",
        "- **Pythonic Philosophy**: Embrace Python's idioms for cleaner, more maintainable code\n",
        "- **Real-world Application**: These patterns solve common programming challenges elegantly\n",
        "- **Performance Benefits**: Many Pythonic patterns offer performance advantages\n",
        "- **Integration**: Pythonic code works seamlessly with Python's built-in functions and libraries\n",
        "- **Readability**: Following Python idioms makes code more readable to other Python developers"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}