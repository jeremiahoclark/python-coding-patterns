{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeremiahoclark/python-coding-patterns/blob/main/05_data_processing_analysis_patterns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eger0BrzRUpo"
      },
      "source": [
        "# Data Processing and Analysis Patterns\n",
        "\n",
        "This notebook covers essential patterns for data processing and analysis in Python, including ETL pipelines, vectorization techniques, and memory-efficient streaming patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMwlbaUCRUpp"
      },
      "source": [
        "## 1. ETL Pipeline Pattern\n",
        "\n",
        "The ETL (Extract, Transform, Load) pipeline pattern provides a structured approach to data processing workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-QdHLxfRUpp"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator, List, Dict, Any, Callable\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "class ETLPipeline:\n",
        "    def __init__(self):\n",
        "        self.extractors = []\n",
        "        self.transformers = []\n",
        "        self.loaders = []\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def add_extractor(self, extractor: Callable):\n",
        "        self.extractors.append(extractor)\n",
        "        return self\n",
        "\n",
        "    def add_transformer(self, transformer: Callable):\n",
        "        self.transformers.append(transformer)\n",
        "        return self\n",
        "\n",
        "    def add_loader(self, loader: Callable):\n",
        "        self.loaders.append(loader)\n",
        "        return self\n",
        "\n",
        "    def execute(self):\n",
        "        data = None\n",
        "\n",
        "        # Extract phase\n",
        "        for extractor in self.extractors:\n",
        "            try:\n",
        "                extracted_data = extractor()\n",
        "                data = extracted_data if data is None else self._merge_data(data, extracted_data)\n",
        "                self.logger.info(f\"Extraction completed: {extractor.__name__}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Extraction failed: {extractor.__name__} - {e}\")\n",
        "                raise\n",
        "\n",
        "        # Transform phase\n",
        "        for transformer in self.transformers:\n",
        "            try:\n",
        "                data = transformer(data)\n",
        "                self.logger.info(f\"Transformation completed: {transformer.__name__}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Transformation failed: {transformer.__name__} - {e}\")\n",
        "                raise\n",
        "\n",
        "        # Load phase\n",
        "        for loader in self.loaders:\n",
        "            try:\n",
        "                loader(data)\n",
        "                self.logger.info(f\"Loading completed: {loader.__name__}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Loading failed: {loader.__name__} - {e}\")\n",
        "                raise\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _merge_data(self, data1, data2):\n",
        "        if isinstance(data1, pd.DataFrame) and isinstance(data2, pd.DataFrame):\n",
        "            return pd.concat([data1, data2], ignore_index=True)\n",
        "        elif isinstance(data1, list) and isinstance(data2, list):\n",
        "            return data1 + data2\n",
        "        else:\n",
        "            return [data1, data2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9Hs79i0RUpp"
      },
      "outputs": [],
      "source": [
        "# Example usage of ETL Pipeline\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Sample data extractors\n",
        "def extract_sales_data():\n",
        "    \"\"\"Extract sales data from source\"\"\"\n",
        "    return pd.DataFrame({\n",
        "        'date': [datetime.now() - timedelta(days=i) for i in range(10)],\n",
        "        'product': [f'Product_{i%3}' for i in range(10)],\n",
        "        'sales': [random.randint(100, 1000) for _ in range(10)],\n",
        "        'region': [random.choice(['North', 'South', 'East', 'West']) for _ in range(10)]\n",
        "    })\n",
        "\n",
        "def extract_inventory_data():\n",
        "    \"\"\"Extract inventory data from source\"\"\"\n",
        "    return pd.DataFrame({\n",
        "        'product': ['Product_0', 'Product_1', 'Product_2'],\n",
        "        'stock': [500, 300, 750],\n",
        "        'cost': [10.5, 15.2, 8.9]\n",
        "    })\n",
        "\n",
        "# Sample transformers\n",
        "def clean_and_validate(data):\n",
        "    \"\"\"Clean and validate the data\"\"\"\n",
        "    if isinstance(data, list) and len(data) == 2:\n",
        "        sales_df, inventory_df = data\n",
        "        # Merge sales and inventory data\n",
        "        merged = sales_df.merge(inventory_df, on='product', how='left')\n",
        "        # Remove any rows with missing data\n",
        "        cleaned = merged.dropna()\n",
        "        return cleaned\n",
        "    return data\n",
        "\n",
        "def calculate_metrics(data):\n",
        "    \"\"\"Calculate business metrics\"\"\"\n",
        "    data = data.copy()\n",
        "    data['revenue'] = data['sales'] * data['cost']\n",
        "    data['profit_margin'] = (data['revenue'] - (data['sales'] * data['cost'] * 0.7)) / data['revenue']\n",
        "    return data\n",
        "\n",
        "# Sample loaders\n",
        "def save_to_csv(data):\n",
        "    \"\"\"Save data to CSV file\"\"\"\n",
        "    data.to_csv('/tmp/processed_sales_data.csv', index=False)\n",
        "    print(f\"Data saved to CSV: {len(data)} rows\")\n",
        "\n",
        "def print_summary(data):\n",
        "    \"\"\"Print data summary\"\"\"\n",
        "    print(\"\\nData Summary:\")\n",
        "    print(f\"Total rows: {len(data)}\")\n",
        "    print(f\"Total revenue: ${data['revenue'].sum():.2f}\")\n",
        "    print(f\"Average profit margin: {data['profit_margin'].mean():.2%}\")\n",
        "\n",
        "# Create and execute pipeline\n",
        "pipeline = (ETLPipeline()\n",
        "           .add_extractor(extract_sales_data)\n",
        "           .add_extractor(extract_inventory_data)\n",
        "           .add_transformer(clean_and_validate)\n",
        "           .add_transformer(calculate_metrics)\n",
        "           .add_loader(save_to_csv)\n",
        "           .add_loader(print_summary))\n",
        "\n",
        "# Execute the pipeline\n",
        "result = pipeline.execute()\n",
        "print(\"\\nFirst 5 rows of processed data:\")\n",
        "print(result.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR6iGcFpRUpq"
      },
      "source": [
        "## 2. Vectorization and Batch Processing\n",
        "\n",
        "Vectorization patterns leverage NumPy and pandas for efficient batch operations on large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GmmBHFLRUpq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from typing import Union, Callable\n",
        "\n",
        "class VectorizedProcessor:\n",
        "    \"\"\"Demonstrates vectorized operations for efficient data processing\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def scalar_operation(data: list, operation: Callable) -> list:\n",
        "        \"\"\"Slow scalar operation for comparison\"\"\"\n",
        "        return [operation(x) for x in data]\n",
        "\n",
        "    @staticmethod\n",
        "    def vectorized_operation(data: np.ndarray, operation: Callable) -> np.ndarray:\n",
        "        \"\"\"Fast vectorized operation\"\"\"\n",
        "        return operation(data)\n",
        "\n",
        "    @staticmethod\n",
        "    def batch_process(data: np.ndarray, batch_size: int, operation: Callable) -> np.ndarray:\n",
        "        \"\"\"Process data in batches to manage memory\"\"\"\n",
        "        results = []\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i+batch_size]\n",
        "            batch_result = operation(batch)\n",
        "            results.append(batch_result)\n",
        "        return np.concatenate(results)\n",
        "\n",
        "# Performance comparison example\n",
        "def complex_calculation(x):\n",
        "    \"\"\"A complex mathematical operation\"\"\"\n",
        "    return np.sqrt(x**2 + np.sin(x) * np.cos(x))\n",
        "\n",
        "# Generate test data\n",
        "size = 1000000\n",
        "test_data_list = list(range(size))\n",
        "test_data_array = np.array(test_data_list, dtype=np.float64)\n",
        "\n",
        "print(f\"Processing {size:,} numbers...\\n\")\n",
        "\n",
        "# Scalar operation (slow)\n",
        "start_time = time.time()\n",
        "scalar_result = VectorizedProcessor.scalar_operation(test_data_list[:10000], complex_calculation)  # Only 10k for demo\n",
        "scalar_time = time.time() - start_time\n",
        "print(f\"Scalar operation (10k items): {scalar_time:.4f} seconds\")\n",
        "\n",
        "# Vectorized operation (fast)\n",
        "start_time = time.time()\n",
        "vectorized_result = VectorizedProcessor.vectorized_operation(test_data_array, complex_calculation)\n",
        "vectorized_time = time.time() - start_time\n",
        "print(f\"Vectorized operation (1M items): {vectorized_time:.4f} seconds\")\n",
        "\n",
        "# Batch processing\n",
        "start_time = time.time()\n",
        "batch_result = VectorizedProcessor.batch_process(test_data_array, 50000, complex_calculation)\n",
        "batch_time = time.time() - start_time\n",
        "print(f\"Batch processing (1M items, 50k batch): {batch_time:.4f} seconds\")\n",
        "\n",
        "print(f\"\\nSpeedup factor: {(scalar_time * 100):.1f}x faster with vectorization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhyJ3DCjRUpq"
      },
      "outputs": [],
      "source": [
        "# Pandas vectorization examples\n",
        "class PandasVectorization:\n",
        "    \"\"\"Demonstrates pandas vectorization techniques\"\"\"\n",
        "\n",
        "    def __init__(self, size: int = 100000):\n",
        "        # Create sample dataset\n",
        "        self.df = pd.DataFrame({\n",
        "            'user_id': np.random.randint(1, 10000, size),\n",
        "            'age': np.random.randint(18, 80, size),\n",
        "            'income': np.random.randint(30000, 200000, size),\n",
        "            'spending': np.random.randint(1000, 50000, size),\n",
        "            'category': np.random.choice(['A', 'B', 'C', 'D'], size),\n",
        "            'timestamp': pd.date_range('2023-01-01', periods=size, freq='1min')\n",
        "        })\n",
        "\n",
        "    def slow_apply_method(self) -> pd.DataFrame:\n",
        "        \"\"\"Slow row-by-row processing\"\"\"\n",
        "        def calculate_score(row):\n",
        "            base_score = row['spending'] / row['income']\n",
        "            age_factor = 1.0 if row['age'] < 30 else 0.8 if row['age'] < 50 else 0.6\n",
        "            category_bonus = {'A': 1.2, 'B': 1.1, 'C': 1.0, 'D': 0.9}[row['category']]\n",
        "            return base_score * age_factor * category_bonus\n",
        "\n",
        "        result = self.df.copy()\n",
        "        result['score'] = result.apply(calculate_score, axis=1)\n",
        "        return result\n",
        "\n",
        "    def fast_vectorized_method(self) -> pd.DataFrame:\n",
        "        \"\"\"Fast vectorized processing\"\"\"\n",
        "        result = self.df.copy()\n",
        "\n",
        "        # Vectorized calculations\n",
        "        base_score = result['spending'] / result['income']\n",
        "\n",
        "        # Vectorized conditional logic\n",
        "        age_factor = np.where(result['age'] < 30, 1.0,\n",
        "                             np.where(result['age'] < 50, 0.8, 0.6))\n",
        "\n",
        "        # Vectorized mapping\n",
        "        category_mapping = {'A': 1.2, 'B': 1.1, 'C': 1.0, 'D': 0.9}\n",
        "        category_bonus = result['category'].map(category_mapping)\n",
        "\n",
        "        result['score'] = base_score * age_factor * category_bonus\n",
        "        return result\n",
        "\n",
        "    def demonstrate_performance(self):\n",
        "        \"\"\"Compare performance of different approaches\"\"\"\n",
        "        print(f\"Dataset size: {len(self.df):,} rows\\n\")\n",
        "\n",
        "        # Test apply method (on smaller subset for demo)\n",
        "        small_processor = PandasVectorization(10000)\n",
        "        start_time = time.time()\n",
        "        apply_result = small_processor.slow_apply_method()\n",
        "        apply_time = time.time() - start_time\n",
        "        print(f\"Apply method (10k rows): {apply_time:.4f} seconds\")\n",
        "\n",
        "        # Test vectorized method\n",
        "        start_time = time.time()\n",
        "        vectorized_result = self.fast_vectorized_method()\n",
        "        vectorized_time = time.time() - start_time\n",
        "        print(f\"Vectorized method (100k rows): {vectorized_time:.4f} seconds\")\n",
        "\n",
        "        print(f\"\\nEstimated speedup: ~{(apply_time * 10):.1f}x faster\")\n",
        "\n",
        "        return vectorized_result\n",
        "\n",
        "# Demonstrate pandas vectorization\n",
        "processor = PandasVectorization()\n",
        "result = processor.demonstrate_performance()\n",
        "print(\"\\nSample results:\")\n",
        "print(result[['age', 'income', 'spending', 'category', 'score']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj-j-QHGRUpq"
      },
      "source": [
        "## 3. Memory Efficiency and Streaming Patterns\n",
        "\n",
        "These patterns help process large datasets that don't fit in memory by using generators and streaming techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEualPkqRUpq"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "from typing import Iterator, Generator, Any\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "class StreamingProcessor:\n",
        "    \"\"\"Memory-efficient streaming data processor\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def read_large_file(file_path: str, chunk_size: int = 8192) -> Generator[str, None, None]:\n",
        "        \"\"\"Read large file in chunks to avoid memory issues\"\"\"\n",
        "        with open(file_path, 'r') as file:\n",
        "            while True:\n",
        "                chunk = file.read(chunk_size)\n",
        "                if not chunk:\n",
        "                    break\n",
        "                yield chunk\n",
        "\n",
        "    @staticmethod\n",
        "    def read_csv_streaming(file_path: str) -> Generator[Dict[str, Any], None, None]:\n",
        "        \"\"\"Stream CSV rows one at a time\"\"\"\n",
        "        with open(file_path, 'r') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                yield row\n",
        "\n",
        "    @staticmethod\n",
        "    def process_streaming_data(data_stream: Iterator,\n",
        "                             processor_func: Callable,\n",
        "                             batch_size: int = 1000) -> Generator[Any, None, None]:\n",
        "        \"\"\"Process streaming data in batches\"\"\"\n",
        "        batch = []\n",
        "        for item in data_stream:\n",
        "            batch.append(item)\n",
        "            if len(batch) >= batch_size:\n",
        "                yield processor_func(batch)\n",
        "                batch = []\n",
        "\n",
        "        # Process remaining items\n",
        "        if batch:\n",
        "            yield processor_func(batch)\n",
        "\n",
        "    @staticmethod\n",
        "    def windowed_processing(data_stream: Iterator, window_size: int) -> Generator[List, None, None]:\n",
        "        \"\"\"Process data in sliding windows\"\"\"\n",
        "        window = []\n",
        "        for item in data_stream:\n",
        "            window.append(item)\n",
        "            if len(window) > window_size:\n",
        "                window.pop(0)\n",
        "            if len(window) == window_size:\n",
        "                yield window.copy()\n",
        "\n",
        "# Create sample data for demonstration\n",
        "def create_sample_csv(file_path: str, num_rows: int = 100000):\n",
        "    \"\"\"Create a sample CSV file for streaming demonstration\"\"\"\n",
        "    with open(file_path, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['id', 'value', 'category', 'timestamp'])\n",
        "        for i in range(num_rows):\n",
        "            writer.writerow([\n",
        "                i,\n",
        "                random.randint(1, 1000),\n",
        "                random.choice(['A', 'B', 'C']),\n",
        "                f'2023-01-{(i % 30) + 1:02d}'\n",
        "            ])\n",
        "\n",
        "# Demonstration\n",
        "temp_file = '/tmp/sample_data.csv'\n",
        "create_sample_csv(temp_file, 50000)\n",
        "print(f\"Created sample file: {temp_file}\")\n",
        "print(f\"File size: {os.path.getsize(temp_file) / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFxQcs4-RUpq"
      },
      "outputs": [],
      "source": [
        "# Streaming processing examples\n",
        "def aggregate_batch(batch: List[Dict]) -> Dict:\n",
        "    \"\"\"Aggregate a batch of records\"\"\"\n",
        "    if not batch:\n",
        "        return {}\n",
        "\n",
        "    total_value = sum(int(row['value']) for row in batch)\n",
        "    categories = {}\n",
        "    for row in batch:\n",
        "        cat = row['category']\n",
        "        categories[cat] = categories.get(cat, 0) + 1\n",
        "\n",
        "    return {\n",
        "        'batch_size': len(batch),\n",
        "        'total_value': total_value,\n",
        "        'avg_value': total_value / len(batch),\n",
        "        'categories': categories\n",
        "    }\n",
        "\n",
        "# Process the CSV file in streaming fashion\n",
        "print(\"Processing CSV file in streaming mode...\")\n",
        "stream = StreamingProcessor.read_csv_streaming(temp_file)\n",
        "processed_batches = StreamingProcessor.process_streaming_data(\n",
        "    stream, aggregate_batch, batch_size=5000\n",
        ")\n",
        "\n",
        "# Collect results\n",
        "batch_count = 0\n",
        "total_records = 0\n",
        "grand_total_value = 0\n",
        "\n",
        "for batch_result in processed_batches:\n",
        "    batch_count += 1\n",
        "    total_records += batch_result['batch_size']\n",
        "    grand_total_value += batch_result['total_value']\n",
        "\n",
        "    if batch_count <= 3:  # Show first 3 batches\n",
        "        print(f\"\\nBatch {batch_count}:\")\n",
        "        print(f\"  Records: {batch_result['batch_size']}\")\n",
        "        print(f\"  Average value: {batch_result['avg_value']:.2f}\")\n",
        "        print(f\"  Categories: {batch_result['categories']}\")\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"Total batches processed: {batch_count}\")\n",
        "print(f\"Total records: {total_records:,}\")\n",
        "print(f\"Grand total value: {grand_total_value:,}\")\n",
        "print(f\"Overall average: {grand_total_value / total_records:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-o9BHipRUpq"
      },
      "outputs": [],
      "source": [
        "# Advanced streaming pattern: Real-time data processing\n",
        "class RealTimeProcessor:\n",
        "    \"\"\"Simulates real-time data processing with memory efficiency\"\"\"\n",
        "\n",
        "    def __init__(self, window_size: int = 100):\n",
        "        self.window_size = window_size\n",
        "        self.metrics = {\n",
        "            'total_processed': 0,\n",
        "            'current_avg': 0,\n",
        "            'peak_value': 0,\n",
        "            'anomaly_count': 0\n",
        "        }\n",
        "\n",
        "    def process_data_stream(self, data_generator: Generator) -> Generator[Dict, None, None]:\n",
        "        \"\"\"Process streaming data with sliding window analytics\"\"\"\n",
        "        window = []\n",
        "\n",
        "        for data_point in data_generator:\n",
        "            value = float(data_point['value'])\n",
        "\n",
        "            # Update sliding window\n",
        "            window.append(value)\n",
        "            if len(window) > self.window_size:\n",
        "                window.pop(0)\n",
        "\n",
        "            # Update metrics\n",
        "            self.metrics['total_processed'] += 1\n",
        "            self.metrics['current_avg'] = sum(window) / len(window)\n",
        "            self.metrics['peak_value'] = max(self.metrics['peak_value'], value)\n",
        "\n",
        "            # Detect anomalies (values > 2 std devs from mean)\n",
        "            if len(window) >= 10:\n",
        "                window_mean = sum(window) / len(window)\n",
        "                window_std = (sum((x - window_mean) ** 2 for x in window) / len(window)) ** 0.5\n",
        "                if abs(value - window_mean) > 2 * window_std:\n",
        "                    self.metrics['anomaly_count'] += 1\n",
        "\n",
        "            # Yield results periodically\n",
        "            if self.metrics['total_processed'] % 1000 == 0:\n",
        "                yield {\n",
        "                    'timestamp': data_point.get('timestamp', 'unknown'),\n",
        "                    'current_value': value,\n",
        "                    'window_avg': self.metrics['current_avg'],\n",
        "                    'total_processed': self.metrics['total_processed'],\n",
        "                    'anomaly_count': self.metrics['anomaly_count'],\n",
        "                    'peak_value': self.metrics['peak_value']\n",
        "                }\n",
        "\n",
        "# Demonstrate real-time processing\n",
        "print(\"\\nDemonstrating real-time streaming processing...\")\n",
        "processor = RealTimeProcessor(window_size=50)\n",
        "data_stream = StreamingProcessor.read_csv_streaming(temp_file)\n",
        "\n",
        "results_count = 0\n",
        "for result in processor.process_data_stream(data_stream):\n",
        "    results_count += 1\n",
        "    if results_count <= 5:  # Show first 5 periodic results\n",
        "        print(f\"\\nResult {results_count}:\")\n",
        "        print(f\"  Processed: {result['total_processed']:,} records\")\n",
        "        print(f\"  Current value: {result['current_value']}\")\n",
        "        print(f\"  Window average: {result['window_avg']:.2f}\")\n",
        "        print(f\"  Anomalies detected: {result['anomaly_count']}\")\n",
        "        print(f\"  Peak value: {result['peak_value']}\")\n",
        "\n",
        "print(f\"\\nFinal metrics:\")\n",
        "print(f\"Total processed: {processor.metrics['total_processed']:,}\")\n",
        "print(f\"Final average: {processor.metrics['current_avg']:.2f}\")\n",
        "print(f\"Peak value: {processor.metrics['peak_value']}\")\n",
        "print(f\"Total anomalies: {processor.metrics['anomaly_count']}\")\n",
        "\n",
        "# Clean up\n",
        "os.remove(temp_file)\n",
        "print(f\"\\nCleaned up temporary file: {temp_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2KsWVaqRUpq"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **ETL Pipeline Pattern**: Provides structured approach to data workflows with proper error handling and logging\n",
        "2. **Vectorization**: Can provide 10-100x performance improvements for mathematical operations on large datasets\n",
        "3. **Streaming Processing**: Essential for handling datasets larger than available memory\n",
        "4. **Memory Management**: Use generators and iterators to process data efficiently without loading everything into memory\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Create an ETL pipeline that processes multiple CSV files and combines them into a single output\n",
        "2. Compare the performance of pandas `.apply()` vs vectorized operations on a large dataset\n",
        "3. Implement a streaming word count algorithm that processes a large text file\n",
        "4. Build a real-time anomaly detection system using sliding window statistics\n",
        "5. Create a memory-efficient data aggregation system that processes data in configurable batch sizes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}